#!/bin/bash
################################################################################
#  A simple Scala based example for Spark
#  Designed to run on SDSC's Comet resource.
#  Mahidhar Tatineni, San Diego Supercomputer Center Feb 2019
################################################################################
#SBATCH --job-name="pagerank-pyspark"
#SBATCH --output="pagerank-pyspark.%j.%N.out"
#SBATCH --partition=compute
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=24
#SBATCH --export=ALL
#SBATCH -t 00:30:00

### Environment setup for Hadoop and Spark
export MODULEPATH=/share/apps/compute/modulefiles/applications:$MODULEPATH
module load spark/2.4.0
module load python
export HADOOP_CONF_DIR=$HOME/mycluster.conf
export WORKDIR=`pwd`


### Sleep for prolog race condition
sleep 30s
myhadoop-configure.sh

### Start HDFS.  Starting YARN isn't necessary since Spark will be running in
### standalone mode on our cluster.
start-dfs.sh

### Load in the necessary Spark environment variables
source $HADOOP_CONF_DIR/spark/spark-env.sh

### Start the Spark masters and workers.  Do NOT use the start-all.sh provided 
### by Spark, as they do not correctly honor $SPARK_CONF_DIR
myspark start
hdfs dfs -mkdir -p /user/input
hdfs dfs -put $SLURM_SUBMIT_DIR/pagerank_data.txt /user/input/pagerank_data.txt
spark-submit $SLURM_SUBMIT_DIR/pagerank.py /user/input/pagerank_data.txt 10

### Shut down Spark and HDFS
myspark stop
stop-dfs.sh

### Clean up
myhadoop-cleanup.sh
